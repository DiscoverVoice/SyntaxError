{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3.5-mini-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import vllm\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_llm(model_name, prompt):\n",
    "    llm = vllm.LLM(model=model_name, \n",
    "                   max_model_len=2048,\n",
    "                   gpu_memory_utilization=0.6)\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        temperature=0.3,\n",
    "        top_p=0.95,\n",
    "        max_tokens=1024,\n",
    "        stop=[\"</s>\", \"Human:\", \"Assistant:\"]\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    generated_code = outputs[0].outputs[0].text\n",
    "    return generated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # system_prompt = \"\\nYou are QA model.\\n\"\n",
    "    system_prompt = \"You are Specification Analyzer\"\n",
    "    problem_description = \"\"\"\n",
    "    B. Kar Salesman\n",
    "    time limit per test: 1 second\n",
    "    memory limit per test: 256 megabytes\n",
    "\n",
    "    Karel is a salesman in a car dealership. The dealership has n different models of cars. There are a_i cars of the i-th model. Karel is an excellent salesperson and can convince customers to buy up to x cars (of Karel's choice), as long as the cars are from different models.\n",
    "\n",
    "    Determine the minimum number of customers Karel has to bring in to sell all the cars.\n",
    "\n",
    "    Input:\n",
    "    Each test contains multiple test cases. The first line contains the number of test cases t (1 ≤ t ≤ 10^4). The description of the test cases follows.\n",
    "\n",
    "    The first line of each test case contains two integers n and x (1 ≤ n ≤ 5⋅10^5; 1 ≤ x ≤ 10) — the number of different models of cars and the maximum number of cars Karel can convince a customer to buy.\n",
    "\n",
    "    The second line contains n integers a1, a2, …, an (1 ≤ a_i ≤ 10^9) — the number of cars of each model.\n",
    "\n",
    "    It is guaranteed that the sum of n over all test cases does not exceed 5⋅10^5.\n",
    "\n",
    "    Output:\n",
    "    For each test case, output the minimum possible number of customers needed to sell all the cars.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_sections = \"\"\"\n",
    "        1. Input and output variables with constraints: \\n\n",
    "        2. Definition of problem: \\n\n",
    "        3. Time and memory constraints: \\n\n",
    "        4. Test case:\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\\nSystem Prompt: {system_prompt}\\nSpecification: {problem_description}\\nAnswer the following questions and don't make a code.\\nQuestion: {prompt_sections}\\nAnswer: \"\"\"\n",
    "    output = generate_text_with_llm(model_name, prompt)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 04:42:11 config.py:107] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 11-01 04:42:13 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='microsoft/Phi-3.5-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/Phi-3.5-mini-instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 11-01 04:42:13 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 11-01 04:42:13 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jieungkim/.cache/pypoetry/virtualenvs/syntaxerror-ow8kG5ih-py3.12/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/jieungkim/.cache/pypoetry/virtualenvs/syntaxerror-ow8kG5ih-py3.12/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 04:42:13 model_runner.py:1056] Starting to load model microsoft/Phi-3.5-mini-instruct...\n",
      "INFO 11-01 04:42:13 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 11-01 04:42:13 selector.py:115] Using XFormers backend.\n",
      "INFO 11-01 04:42:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc0b3cfa1d74fe6be06a9894d1d8bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 04:42:15 model_runner.py:1067] Loading model weights took 7.1659 GB\n",
      "INFO 11-01 04:42:15 gpu_executor.py:122] # GPU blocks: 1108, # CPU blocks: 682\n",
      "INFO 11-01 04:42:15 gpu_executor.py:126] Maximum concurrency for 2048 tokens per request: 8.66x\n",
      "INFO 11-01 04:42:16 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-01 04:42:16 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-01 04:42:22 model_runner.py:1523] Graph capturing finished in 6 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.53s/it, est. speed input: 91.56 toks/s, output: 93.10 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Input and output variables with constraints:\n",
      "\n",
      "- Input:\n",
      "    - t: The number of test cases (1 ≤ t ≤ 10^4)\n",
      "    - n: The number of different car models (1 ≤ n ≤ 5*10^5)\n",
      "    - x: The maximum number of cars Karel can sell to a single customer (1 ≤ x ≤ 10)\n",
      "    - a: An array of integers representing the number of cars of each model (1 ≤ a_i ≤ 10^9)\n",
      "\n",
      "- Output:\n",
      "    - The minimum number of customers needed to sell all cars for each test case\n",
      "\n",
      "2. Definition of problem:\n",
      "\n",
      "The problem requires us to find the minimum number of customers Karel needs to convince to buy cars from the dealership, given that he can sell up to x cars per customer and can only sell cars from different models. The goal is to sell all the cars available in the dealership.\n",
      "\n",
      "3. Time and memory constraints:\n",
      "\n",
      "- Time limit per test: 1 second\n",
      "- Memory limit per test: 256 megabytes\n",
      "\n",
      "4. Test case:\n",
      "\n",
      "Example:\n",
      "\n",
      "Input:\n",
      "    t = 2\n",
      "    n = 3\n",
      "    x = 2\n",
      "    a = [3, 4, 5]\n",
      "\n",
      "Output:\n",
      "    2\n",
      "\n",
      "Explanation:\n",
      "\n",
      "- For the first test case, Karel can sell 2 cars from the first model, 2 cars from the second model, and 1 car from the third model, which requires 2 customers.\n",
      "- For the second test case, Karel can sell 2 cars from the first model, 2 cars from the second model, and 3 cars from the third model, which also requires 2 customers.\n",
      "\n",
      "In both cases, the minimum number of customers needed to sell all the cars is 2.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntaxerror-ow8kG5ih-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
